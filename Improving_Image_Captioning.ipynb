{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w64R0FUkTJgd"
      },
      "outputs": [],
      "source": [
        "!pip install torch Pillow matplotlib accelerate bitsandbytes\n",
        "#!pip install textgrad\n",
        "!pip install transformers==4.49.0\n",
        "!pip install torch\n",
        "!pip install torchmetrics\n",
        "!pip install torch_optimizer\n",
        "!pip install hpsv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    VisionEncoderDecoderModel,\n",
        "    ViTImageProcessor,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BlipForConditionalGeneration,\n",
        "    BlipProcessor,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoProcessor,\n",
        "    AutoModelForImageTextToText\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig\n",
        "from huggingface_hub import login\n",
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import io\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# BLIP 모델\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n",
        "\n",
        "# vit-gpt2 모델\n",
        "vit_model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(\"cuda\")\n",
        "vit_processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "vit_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "# Florence-2\n",
        "florence_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Florence-2-large\",#base or large\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16  # 또는 float32\n",
        ").to(device)\n",
        "\n",
        "florence_processor = AutoProcessor.from_pretrained(\n",
        "    \"microsoft/Florence-2-large\",\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "861WXuwtUY6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(image, model_type=\"blip\"):\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    if model_type == \"blip\":\n",
        "        inputs = blip_processor(image, return_tensors=\"pt\").to(\"cuda\")\n",
        "        outputs = blip_model.generate(**inputs, max_length=50)\n",
        "        return blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    elif model_type == \"vit-gpt2\":\n",
        "        pixel_values = vit_processor(images=image, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n",
        "        output_ids = vit_model.generate(pixel_values, max_length=50, num_beams=4)\n",
        "        return vit_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "#textgrad\n",
        "def florence_feedback(caption, image):\n",
        "    prompt = f\"\"\"Image Caption Enhancement Task:\n",
        "Original Caption: {caption}\n",
        "Image Characteristics: {image.size} | Channels: {len(image.getbands())}\n",
        "\n",
        "Improvement Requests:\n",
        "1. Accurately identifying each objects.\n",
        "2. Describing each as if cropped.\n",
        "3. Considering what the image conveys even when seen out of focus.\n",
        "4. Improve natural language flow checking sentence before descriptions.\n",
        "\n",
        "Improved Caption:\"\"\"\n",
        "    # Florence-2는 이미지+텍스트 입력 후 텍스트 생성\n",
        "    inputs = florence_processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
        "    inputs = {k: v.half() if v.dtype == torch.float32 else v for k, v in inputs.items()}\n",
        "    # 실행 속도와 용량을 위해 16bit 짜리 float를 사용\n",
        "    outputs = florence_model.generate(**inputs, max_new_tokens=100)\n",
        "    # Florence-2는 processor.decode가 아니라 tokenizer.decode 사용\n",
        "    improved_caption = florence_processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return improved_caption.strip()"
      ],
      "metadata": {
        "id": "QEqiz12WUurc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#평가 시스템\n",
        "from torchmetrics.multimodal import CLIPScore\n",
        "\n",
        "class CaptionEvaluator:\n",
        "    def __init__(self):\n",
        "        self.clip_metric = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
        "\n",
        "    def evaluate(self, image, caption):\n",
        "        # 이미지 전처리\n",
        "        processed_image = image.resize((224,224))\n",
        "        img_tensor = torch.tensor(np.array(processed_image)).permute(2,0,1).unsqueeze(0)\n",
        "\n",
        "        # CLIP 점수 계산\n",
        "        score = self.clip_metric(img_tensor, [caption])\n",
        "        return score.item()\n",
        "\"\"\"\n",
        "\n",
        "from torchmetrics.multimodal import CLIPScore\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModel, BlipProcessor, BlipForQuestionAnswering\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import hpsv2\n",
        "\n",
        "class CaptionEvaluator:\n",
        "    def __init__(self):\n",
        "        self.clip_metric = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
        "\n",
        "        # HPS-V2 모델 초기화\n",
        "        try:\n",
        "            # HPS-V2는 직접 import하여 사용\n",
        "            import hpsv2\n",
        "            self.hps_available = True\n",
        "        except ImportError:\n",
        "            print(\"HPS-V2 모델을 로드할 수 없습니다. pip install hpsv2 필요\")\n",
        "            self.hps_available = False\n",
        "\n",
        "        # Pick Score 모델 초기화\n",
        "        try:\n",
        "            self.pick_processor = AutoProcessor.from_pretrained(\"yuvalkirstain/PickScore_v1\")\n",
        "            self.pick_model = AutoModel.from_pretrained(\"yuvalkirstain/PickScore_v1\")\n",
        "            self.pick_available = True\n",
        "        except:\n",
        "            print(\"Pick Score 모델을 로드할 수 없습니다.\")\n",
        "            self.pick_available = False\n",
        "\n",
        "        # VQA Score용 BLIP 모델 초기화\n",
        "        try:\n",
        "            self.vqa_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "            self.vqa_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "            self.vqa_available = True\n",
        "        except:\n",
        "            print(\"VQA 모델을 로드할 수 없습니다.\")\n",
        "            self.vqa_available = False\n",
        "\n",
        "    def calculate_hps_score(self, image, caption):\n",
        "        \"\"\"HPS-V2 점수 계산\"\"\"\n",
        "        if not self.hps_available:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            score = hpsv2.score(image, caption, hps_version=\"v2.1\")\n",
        "            return score\n",
        "        except Exception as e:\n",
        "            print(f\"HPS-V2 점수 계산 오류: {e}\")\n",
        "            return None\n",
        "\n",
        "    def calculate_pick_score(self, image, caption):\n",
        "        \"\"\"Pick Score 계산\"\"\"\n",
        "        if not self.pick_available:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # 입력 전처리\n",
        "            inputs = self.pick_processor(\n",
        "                images=image,\n",
        "                text=caption,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.pick_model(**inputs)\n",
        "                # Pick Score는 logits의 첫 번째 값을 사용\n",
        "                score = outputs.logits_per_image.item()\n",
        "\n",
        "            return score\n",
        "        except Exception as e:\n",
        "            print(f\"Pick Score 계산 오류: {e}\")\n",
        "            return None\n",
        "\n",
        "    def calculate_vqa_score(self, image, caption):\n",
        "        \"\"\"VQA Score 계산 (캡션 기반 질문-답변 정확도)\"\"\"\n",
        "        if not self.vqa_available:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # 캡션에서 간단한 질문들 생성\n",
        "            questions = [\n",
        "                \"What is in this image?\",\n",
        "                \"What is the main object in the image?\",\n",
        "                \"What color is the main object?\",\n",
        "                \"What is happening in this image?\"\n",
        "            ]\n",
        "\n",
        "            correct_answers = 0\n",
        "            total_questions = len(questions)\n",
        "\n",
        "            for question in questions:\n",
        "                # VQA 모델로 답변 생성\n",
        "                inputs = self.vqa_processor(image, question, return_tensors=\"pt\")\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.vqa_model.generate(**inputs, max_length=20)\n",
        "                    answer = self.vqa_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "                # 간단한 키워드 매칭으로 정확도 계산\n",
        "                caption_lower = caption.lower()\n",
        "                answer_lower = answer.lower()\n",
        "\n",
        "                # 답변의 주요 단어가 캡션에 포함되어 있는지 확인\n",
        "                answer_words = answer_lower.split()\n",
        "                if any(word in caption_lower for word in answer_words if len(word) > 3):\n",
        "                    correct_answers += 1\n",
        "\n",
        "            vqa_score = correct_answers / total_questions\n",
        "            return vqa_score\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"VQA Score 계산 오류: {e}\")\n",
        "            return None\n",
        "\n",
        "    def evaluate(self, image, caption):\n",
        "        # 이미지 전처리\n",
        "        processed_image = image.resize((224,224))\n",
        "        img_tensor = torch.tensor(np.array(processed_image)).permute(2,0,1).unsqueeze(0)\n",
        "\n",
        "        # CLIP 점수 계산\n",
        "        clip_score = self.clip_metric(img_tensor, [caption])\n",
        "\n",
        "        results = {'clip_score': clip_score.item()}\n",
        "\n",
        "        # HPS-V2 점수 계산\n",
        "        hps_score = self.calculate_hps_score(image, caption)\n",
        "        if hps_score is not None:\n",
        "            results['hps_score'] = hps_score\n",
        "\n",
        "        # Pick Score 계산\n",
        "        pick_score = self.calculate_pick_score(image, caption)\n",
        "        if pick_score is not None:\n",
        "            results['pick_score'] = pick_score\n",
        "\n",
        "        # VQA Score 계산\n",
        "        vqa_score = self.calculate_vqa_score(image, caption)\n",
        "        if vqa_score is not None:\n",
        "            results['vqa_score'] = vqa_score\n",
        "\n",
        "        # 종합 점수 계산 (사용 가능한 점수들의 평균)\n",
        "        available_scores = [score for score in [\n",
        "            results.get('clip_score'),\n",
        "            results.get('hps_score'),\n",
        "            results.get('pick_score'),\n",
        "            results.get('vqa_score')\n",
        "        ] if score is not None]\n",
        "\n",
        "        if available_scores:\n",
        "            results['composite_score'] = sum(available_scores) / len(available_scores)\n",
        "\n",
        "        return results\n",
        "\n"
      ],
      "metadata": {
        "id": "xb0NfMirXfUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fine tunning\n",
        "from torch_optimizer import RAdam\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(device)\n",
        "processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "optimizer = RAdam(model.parameters(), lr=5e-5)\n",
        "# 학습용 데이터셋 구축\n",
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, images, captions=None, processor=None, tokenizer=None):\n",
        "        self.images = images\n",
        "        self.captions = captions or [\"\"] * len(images)\n",
        "        self.processor = processor\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.processor(images=self.images[idx], return_tensors=\"pt\").pixel_values.squeeze()\n",
        "        caption = self.tokenizer(self.captions[idx], padding=\"max_length\", truncation=True, max_length=50, return_tensors=\"pt\").input_ids.squeeze()\n",
        "        return image, caption\n",
        "\n",
        "def fine_tune_model(image, improved_caption):\n",
        "    model.train()\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
        "    labels = tokenizer(improved_caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=50).input_ids.to(device)\n",
        "    outputs = model(pixel_values=inputs, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return loss.item()\n",
        "\n",
        "#이미지 로드\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "# 1. COCO 2017 validation 이미지 zip 파일 다운로드\n",
        "url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
        "zip_path = \"val2017.zip\"\n",
        "img_dir = \"val2017\"\n",
        "\n",
        "if not os.path.exists(zip_path):\n",
        "    print(\"Downloading COCO 2017 val images...\")\n",
        "    r = requests.get(url, stream=True)\n",
        "    with open(zip_path, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "# 2. 압축 해제\n",
        "if not os.path.exists(img_dir):\n",
        "    print(\"Extracting images...\")\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "\n",
        "# 3. 이미지 파일 리스트 가져오기\n",
        "img_files = [os.path.join(img_dir, fname) for fname in os.listdir(img_dir) if fname.endswith(\".jpg\")]\n",
        "\n",
        "# 4. PIL 이미지 리스트 만들기 (최대 5000장)\n",
        "images = []\n",
        "for img_path in img_files[:10]:  # 필요시 [:100] 등으로 조절\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    images.append(img)\n",
        "\n",
        "print(f\"총 {len(images)}장의 이미지를 불러왔습니다.\")\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "to_pil = transforms.ToPILImage()\n",
        "\n",
        "captions = [generate_caption(img, model_type=\"blip\") for img in images]\n",
        "dataset = CaptionDataset(images, captions, processor, tokenizer)\n",
        "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "    for image_tensor, caption in dataset:\n",
        "        image = to_pil(image_tensor)\n",
        "        initial_caption = generate_caption(image)\n",
        "        improved_caption = florence_feedback(initial_caption, image)\n",
        "        fine_tune_model(image, improved_caption)\n"
      ],
      "metadata": {
        "id": "osklKt6rljU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_caption(image, max_iter=3):\n",
        "    evaluator = CaptionEvaluator()\n",
        "\n",
        "    # 초기 캡션 생성\n",
        "    base_caption = generate_caption(image, model_type=\"blip\")\n",
        "    best_score = evaluator.evaluate(image, base_caption)\n",
        "    best_caption = base_caption\n",
        "\n",
        "    #print(f\"초기 캡션: {base_caption} | 점수: {best_score:.2f}\")\n",
        "\n",
        "    # 반복적 개선\n",
        "    for i in range(max_iter):\n",
        "        new_caption = florence_feedback(best_caption, image)\n",
        "        #gradient_caption = florence_feedback(best_caption, image)\n",
        "        #inputs = florence_processor(images=image, text=gradient_caption, return_tensors=\"pt\").to(device)\n",
        "        #inputs = {k: v.half() if v.dtype == torch.float32 else v for k, v in inputs.items()}\n",
        "        #new_caption = florence_model.generate(**inputs, max_new_tokens=200)\n",
        "        current_score = evaluator.evaluate(image, new_caption)\n",
        "\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_caption = new_caption\n",
        "            print(f\"updated at {i+1}: {new_caption} | score: {current_score:.2f}\")\n",
        "\n",
        "    return best_caption"
      ],
      "metadata": {
        "id": "nutpdcz4XkEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 이미지 로드\n",
        "def load_image(url):\n",
        "    response = requests.get(url)\n",
        "    return Image.open(io.BytesIO(response.content))\n",
        "\"\"\"\n",
        "test_image = load_image(\"https://images.unsplash.com/photo-1583512603805-3cc6b41f3edb\")\n",
        "\n",
        "# 최적화 실행\n",
        "final_caption = optimize_caption(test_image)\n",
        "print(\"\\n최종 결과:\", final_caption)\n",
        "\n",
        "# 결과 시각화\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(test_image)\n",
        "plt.title(f\"최적화된 캡션:\\n{final_caption}\", wrap=True)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CI0B7iz5XntE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption_finetuned(image):\n",
        "      device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "      model.eval()\n",
        "      pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
        "      with torch.no_grad():\n",
        "          output_ids = model.generate(pixel_values, max_length=50, num_beams=4)\n",
        "      caption = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
        "      return caption"
      ],
      "metadata": {
        "id": "6qcNmQQRpnyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def comparative_analysis(image):\n",
        "    models = [\"blip\", \"vit-gpt2\"]\n",
        "    results = {}\n",
        "    evaluator = CaptionEvaluator()\n",
        "\n",
        "    for model_import in models:\n",
        "        caption = generate_caption(image, model_import)\n",
        "        result = evaluator.evaluate(image, caption)\n",
        "        results[model_import] = {\"caption\": caption, \"CLIP_Score\": result.get('clip_score', 'N/A'), \"HPS_Score\": result.get('hps_score', 'N/A'), \"Pick_Score\": result.get('pick_score', 'N/A'), \"VQA_Score\": result.get('vqa_score', 'N/A'), \"Composite_Score\": result.get('composite_score', 'N/A')}\n",
        "\n",
        "    banila = optimize_caption(image, max_iter=0)\n",
        "    banila_result = evaluator.evaluate(image, banila)\n",
        "    results[\"Florence-2\"] = {\"caption\": banila, \"CLIP_Score\": banila_result.get('clip_score', 'N/A'), \"HPS_Score\": banila_result.get('hps_score', 'N/A'), \"Pick_Score\": banila_result.get('pick_score', 'N/A'), \"VQA_Score\": banila_result.get('vqa_score', 'N/A'), \"Composite_Score\": banila_result.get('composite_score', 'N/A')}\n",
        "    # Florence 기반 최적화\n",
        "    #optimized = optimize_caption(image, max_iter=10)\n",
        "    #opt_score = evaluator.evaluate(image, optimized)\n",
        "    #results[\"optimized\"] = {\"caption\": optimized, \"score\": opt_score}\n",
        "\n",
        "    # fine-tuned 모델 결과\n",
        "    finetuned_caption = generate_caption_finetuned(image)\n",
        "    finetuned_result = evaluator.evaluate(image, finetuned_caption)\n",
        "    results[\"finetuned\"] = {\"caption\": finetuned_caption, \"CLIP_Score\": finetuned_result.get('clip_score', 'N/A'), \"HPS_Score\": finetuned_result.get('hps_score', 'N/A'), \"Pick_Score\": finetuned_result.get('pick_score', 'N/A'), \"VQA_Score\": finetuned_result.get('vqa_score', 'N/A'), \"Composite_Score\": finetuned_result.get('composite_score', 'N/A')}\n",
        "\n",
        "\n",
        "    # 결과 시각화\n",
        "    fig, axs = plt.subplots(1, len(results) + 1, figsize=(25, 5))\n",
        "    axs[0].imshow(image)\n",
        "    axs[0].set_title(\"original image\")\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    for idx, (name, data) in enumerate(results.items(), 1):\n",
        "        axs[idx].imshow(image)\n",
        "        axs[idx].set_title(f\"{name}\\nCLIP_Score: {data['CLIP_Score']:.2f}\\nPick_Score: {data['Pick_Score']:.2f}\\nVQA_Score: {data['VQA_Score']:.2f}\\nComposite_Score: {data['Composite_Score']:.2f}\\n\")\n",
        "        axs[idx].text(0, -50, data['caption'], wrap=True)\n",
        "        axs[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "uvg4PX3dXqCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = load_image(\"https://pbs.twimg.com/media/Gq_RCtfWUAEsxoC?format=jpg&name=large\")\n",
        "\n",
        "comparative_analysis(test_image)"
      ],
      "metadata": {
        "id": "BP--CjNf98d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = load_image(\"https://pbs.twimg.com/media/GrDxQPibgAAz862?format=jpg&name=medium\")\n",
        "\n",
        "comparative_analysis(test_image)"
      ],
      "metadata": {
        "id": "O1xVwCVES7Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = load_image(\"https://pbs.twimg.com/media/GksdDm_XAAAx705?format=jpg&name=large\")\n",
        "\n",
        "comparative_analysis(test_image)"
      ],
      "metadata": {
        "id": "F7GYcUAAQ_Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = load_image(\"https://pbs.twimg.com/media/GrYrYt3bAAEdp-z?format=jpg&name=large\")\n",
        "\n",
        "comparative_analysis(test_image)"
      ],
      "metadata": {
        "id": "MGgUqSRhb2lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = load_image(\"https://pbs.twimg.com/media/GtVfY2EWcAAdxzS?format=jpg&name=large\")\n",
        "\n",
        "comparative_analysis(test_image)"
      ],
      "metadata": {
        "id": "0-HLMANObZTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = load_image(\"https://pbs.twimg.com/media/GtOTH4wakAElbhy?format=jpg&name=medium\")\n",
        "\n",
        "comparative_analysis(test_image)"
      ],
      "metadata": {
        "id": "WE6ZO62IbY8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = load_image(\"https://pbs.twimg.com/media/GtVL6gnX0AA7zEq?format=jpg&name=large\")\n",
        "\n",
        "comparative_analysis(test_image)"
      ],
      "metadata": {
        "id": "AomhXtx8_rOA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}